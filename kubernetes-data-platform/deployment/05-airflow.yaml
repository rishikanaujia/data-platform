# Apache Airflow 2.8.1 Production Configuration - All Fixes Applied
apiVersion: v1
kind: ConfigMap
metadata:
  name: airflow-config
  namespace: data-platform
  labels:
    app: airflow
    tier: orchestration
data:
  airflow.cfg: |
    [core]
    dags_folder = /opt/airflow/dags
    hostname_callable = airflow.utils.net.get_host_ip_address
    default_timezone = utc
    executor = CeleryExecutor
    parallelism = 32
    max_active_tasks_per_dag = 16
    dags_are_paused_at_creation = True
    max_active_runs_per_dag = 16
    load_examples = False
    plugins_folder = /opt/airflow/plugins
    donot_pickle = False
    dagbag_import_timeout = 30
    task_runner = StandardTaskRunner
    default_impersonation =
    security =
    unit_test_mode = False
    enable_xcom_pickling = False
    killed_task_cleanup_time = 60
    dag_discovery_safe_mode = True
    default_task_retries = 0
    min_serialized_dag_update_interval = 30
    min_serialized_dag_fetch_interval = 10
    max_serialized_dag_update_interval = 180
    compress_serialized_dags = False

    [database]
    sql_alchemy_conn = postgresql+psycopg2://postgres:jXtMzONfBiGNC9SijLmFGmJGydFEQupM@postgres-primary:5432/airflow
    sql_alchemy_max_overflow = 20
    sql_alchemy_pool_recycle = 1800
    sql_alchemy_pool_pre_ping = True
    sql_alchemy_schema =

    [celery]
    broker_url = redis://:YX4sDK5Lpz10SNi4iGJnFTohIuHfzuhh@redis:6379/1
    result_backend = redis://:YX4sDK5Lpz10SNi4iGJnFTohIuHfzuhh@redis:6379/1
    flower_host = 0.0.0.0
    flower_url_prefix =
    flower_port = 5555
    flower_basic_auth =
    default_queue = default
    sync =
    celery_app_name = airflow.executors.celery_executor
    worker_concurrency = 16
    worker_log_server_port = 8793
    broker_transport_options = {"visibility_timeout": 21600}
    event_buffer_size = 1000
    worker_enable_remote_control = true
    result_expires = 3600
    task_track_started = True
    task_publish_retry = True
    worker_prefetch_multiplier = 1

    [operators]
    default_owner = airflow
    default_cpus = 1
    default_ram = 512
    default_disk = 512
    default_gpus = 0
    allow_illegal_arguments = False

    [webserver]
    base_url = http://localhost:8080
    default_ui_timezone = UTC
    web_server_host = 0.0.0.0
    web_server_port = 8080
    web_server_ssl_cert =
    web_server_ssl_key =
    web_server_master_timeout = 120
    web_server_worker_timeout = 120
    worker_refresh_batch_size = 1
    worker_refresh_interval = 6000
    workers = 4
    worker_class = sync
    access_logfile = -
    error_logfile = -
    expose_config = False
    authenticate = False
    filter_by_owner = False
    owner_mode = user
    dag_default_view = tree
    dag_orientation = LR
    demo_mode = False
    log_fetch_timeout_sec = 5
    hide_paused_dags_by_default = False
    page_size = 100
    navbar_color = #fff
    default_dag_run_display_number = 25
    enable_proxy_fix = False
    proxy_fix_x_for = 1
    proxy_fix_x_proto = 1
    proxy_fix_x_host = 1
    proxy_fix_x_port = 1
    proxy_fix_x_prefix = 1
    cookie_secure = False
    cookie_samesite = Lax
    default_wrap = False
    x_frame_enabled = True
    show_recent_stats_for_completed_runs = True
    update_fab_perms = True

    [email]
    email_backend = airflow.utils.email.send_email_smtp
    email_conn_id = smtp_default
    default_email_on_retry = True
    default_email_on_failure = True

    [smtp]
    smtp_host = localhost
    smtp_starttls = True
    smtp_ssl = False
    smtp_user =
    smtp_password =
    smtp_port = 587
    smtp_mail_from = airflow@dataplatform.local
    smtp_timeout = 30
    smtp_retry_limit = 5

    [logging]
    base_log_folder = /opt/airflow/logs
    remote_logging = False
    remote_log_conn_id =
    remote_base_log_folder =
    encrypt_s3_logs = False
    logging_level = INFO
    fab_logging_level = WARN
    logging_config_class =
    colored_console_log = True
    colored_log_format = [%%(blue)s%%(asctime)s%%(reset)s] {{%%(blue)s%%(filename)s:%%(reset)s%%(lineno)d}} %%(log_color)s%%(levelname)s%%(reset)s - %%(log_color)s%%(message)s%%(reset)s
    colored_formatter_class = airflow.utils.log.colored_log.CustomTTYColoredFormatter
    log_format = [%%(asctime)s] {{%%(filename)s:%%(lineno)d}} %%(levelname)s - %%(message)s
    simple_log_format = %%(asctime)s %%(levelname)s - %%(message)s
    task_log_prefix_template =
    log_filename_template = {{ ti.dag_id }}/{{ ti.task_id }}/{{ ts }}/{{ try_number }}.log
    log_processor_filename_template = {{ filename }}.log
    dag_processor_manager_log_location = /opt/airflow/logs/dag_processor_manager/dag_processor_manager.log
    task_log_reader = task

    [metrics]
    statsd_on = False
    statsd_host = localhost
    statsd_port = 8125
    statsd_prefix = airflow
    statsd_allow_list =
    stat_name_handler =
    statsd_datadog_enabled = False
    statsd_datadog_tags =
    statsd_custom_client_path =

    [secrets]
    backend =
    backend_kwargs =

    [cli]
    api_client = airflow.api.client.json_client
    endpoint_url = http://localhost:8080

    [debug]
    fail_fast = False

    [api]
    enable_experimental_api = False
    auth_backends = airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session
    maximum_page_limit = 100
    fallback_page_limit = 100

    [admin]
    hide_sensitive_variable_fields = True
    sensitive_variable_fields =

---
apiVersion: v1
kind: Secret
metadata:
  name: airflow-secret
  namespace: data-platform
  labels:
    app: airflow
type: Opaque
data:
  fernet-key: TGd3ZVBrdXhCWUNoRW9ZaHBKVzQwYU9adWNXRHFrQ2VmUW04Y2YtVFJnQT0=
  webserver-secret-key: ZmM0M2FlNDgwMzY3NDkyNTg0Y2E3ZDBjYTljYzA0NGFjZTViOTQ5M2Q1MzExNWQxYzMxZmNkMjBkOWYxZjg3Yg==

---
apiVersion: batch/v1
kind: Job
metadata:
  name: airflow-db-init-fixed
  namespace: data-platform
  labels:
    app: airflow
    job: db-init
spec:
  template:
    metadata:
      labels:
        app: airflow
        job: db-init
    spec:
      initContainers:
      - name: wait-for-postgres
        image: busybox:1.35
        command:
        - /bin/sh
        - -c
        - |
          echo "Waiting for PostgreSQL to be ready..."
          until nc -z postgres-primary 5432; do
            echo "PostgreSQL not ready, waiting..."
            sleep 5
          done
          echo "PostgreSQL is ready!"
      containers:
      - name: airflow-db-init
        image: apache/airflow:2.8.1-python3.11
        command:
        - /bin/bash
        - -c
        - |
          # Initialize database
          airflow db init

          # Create admin user
          airflow users create             --username admin             --firstname Admin             --lastname User             --role Admin             --email admin@dataplatform.local             --password "zFq94ot53CQNWWAK"
        env:
        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN
          value: "postgresql+psycopg2://postgres:password123@postgres-primary:5432/airflow"
        - name: AIRFLOW__CORE__FERNET_KEY
          valueFrom:
            secretKeyRef:
              name: airflow-secret
              key: fernet-key
        - name: AIRFLOW__CELERY__BROKER_URL
          value: "redis://:redispassword123@redis:6379/1"
        - name: AIRFLOW__CELERY__RESULT_BACKEND
          value: "redis://:redispassword123@redis:6379/1"
        - name: AIRFLOW__CORE__EXECUTOR
          value: "CeleryExecutor"
        - name: AIRFLOW__CORE__LOAD_EXAMPLES
          value: "False"
        - name: AIRFLOW__WEBSERVER__SECRET_KEY
          valueFrom:
            secretKeyRef:
              name: airflow-secret
              key: webserver-secret-key
        volumeMounts:
        - name: airflow-config
          mountPath: /opt/airflow/airflow.cfg
          subPath: airflow.cfg
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
      volumes:
      - name: airflow-config
        configMap:
          name: airflow-config
      restartPolicy: OnFailure

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: airflow-webserver
  namespace: data-platform
  labels:
    app: airflow
    component: webserver
    tier: orchestration
spec:
  replicas: 1
  selector:
    matchLabels:
      app: airflow
      component: webserver
  template:
    metadata:
      labels:
        app: airflow
        component: webserver
        tier: orchestration
    spec:
      containers:
      - name: airflow-webserver
        image: apache/airflow:2.8.1-python3.11
        command:
        - /bin/bash
        - -c
        - |
          # Start webserver
          airflow webserver --port 8080
        env:
        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN
          value: "postgresql+psycopg2://postgres:password123@postgres-primary:5432/airflow"
        - name: AIRFLOW__CORE__FERNET_KEY
          valueFrom:
            secretKeyRef:
              name: airflow-secret
              key: fernet-key
        - name: AIRFLOW__CELERY__BROKER_URL
          value: "redis://:redispassword123@redis:6379/1"
        - name: AIRFLOW__CELERY__RESULT_BACKEND
          value: "redis://:redispassword123@redis:6379/1"
        - name: AIRFLOW__CORE__EXECUTOR
          value: "CeleryExecutor"
        - name: AIRFLOW__CORE__LOAD_EXAMPLES
          value: "False"
        - name: AIRFLOW__WEBSERVER__SECRET_KEY
          valueFrom:
            secretKeyRef:
              name: airflow-secret
              key: webserver-secret-key
        ports:
        - containerPort: 8080
          name: webserver
        volumeMounts:
        - name: airflow-config
          mountPath: /opt/airflow/airflow.cfg
          subPath: airflow.cfg
        - name: airflow-logs
          mountPath: /opt/airflow/logs
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 120
          periodSeconds: 30
          timeoutSeconds: 10
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 60
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
      volumes:
      - name: airflow-config
        configMap:
          name: airflow-config
      - name: airflow-logs
        persistentVolumeClaim:
          claimName: airflow-logs-pvc

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: airflow-scheduler
  namespace: data-platform
  labels:
    app: airflow
    component: scheduler
    tier: orchestration
spec:
  replicas: 1
  selector:
    matchLabels:
      app: airflow
      component: scheduler
  template:
    metadata:
      labels:
        app: airflow
        component: scheduler
        tier: orchestration
    spec:
      containers:
      - name: airflow-scheduler
        image: apache/airflow:2.8.1-python3.11
        command:
        - /bin/bash
        - -c
        - |
          # Start scheduler
          airflow scheduler
        env:
        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN
          value: "postgresql+psycopg2://postgres:password123@postgres-primary:5432/airflow"
        - name: AIRFLOW__CORE__FERNET_KEY
          valueFrom:
            secretKeyRef:
              name: airflow-secret
              key: fernet-key
        - name: AIRFLOW__CELERY__BROKER_URL
          value: "redis://:redispassword123@redis:6379/1"
        - name: AIRFLOW__CELERY__RESULT_BACKEND
          value: "redis://:redispassword123@redis:6379/1"
        - name: AIRFLOW__CORE__EXECUTOR
          value: "CeleryExecutor"
        - name: AIRFLOW__CORE__LOAD_EXAMPLES
          value: "False"
        - name: AIRFLOW__WEBSERVER__SECRET_KEY
          valueFrom:
            secretKeyRef:
              name: airflow-secret
              key: webserver-secret-key
        volumeMounts:
        - name: airflow-config
          mountPath: /opt/airflow/airflow.cfg
          subPath: airflow.cfg
        - name: airflow-logs
          mountPath: /opt/airflow/logs
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
        livenessProbe:
          exec:
            command:
            - /bin/bash
            - -c
            - "pgrep -f 'airflow scheduler'"
          initialDelaySeconds: 120
          periodSeconds: 30
          timeoutSeconds: 10
          failureThreshold: 3
      volumes:
      - name: airflow-config
        configMap:
          name: airflow-config
      - name: airflow-logs
        persistentVolumeClaim:
          claimName: airflow-logs-pvc

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: airflow-worker
  namespace: data-platform
  labels:
    app: airflow
    component: worker
    tier: orchestration
spec:
  replicas: 2
  selector:
    matchLabels:
      app: airflow
      component: worker
  template:
    metadata:
      labels:
        app: airflow
        component: worker
        tier: orchestration
    spec:
      containers:
      - name: airflow-worker
        image: apache/airflow:2.8.1-python3.11
        command:
        - /bin/bash
        - -c
        - |
          # Start celery worker
          airflow celery worker
        env:
        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN
          value: "postgresql+psycopg2://postgres:password123@postgres-primary:5432/airflow"
        - name: AIRFLOW__CORE__FERNET_KEY
          valueFrom:
            secretKeyRef:
              name: airflow-secret
              key: fernet-key
        - name: AIRFLOW__CELERY__BROKER_URL
          value: "redis://:redispassword123@redis:6379/1"
        - name: AIRFLOW__CELERY__RESULT_BACKEND
          value: "redis://:redispassword123@redis:6379/1"
        - name: AIRFLOW__CORE__EXECUTOR
          value: "CeleryExecutor"
        - name: AIRFLOW__CELERY__WORKER_CONCURRENCY
          value: "16"
        - name: AIRFLOW__WEBSERVER__SECRET_KEY
          valueFrom:
            secretKeyRef:
              name: airflow-secret
              key: webserver-secret-key
        volumeMounts:
        - name: airflow-config
          mountPath: /opt/airflow/airflow.cfg
          subPath: airflow.cfg
        - name: airflow-logs
          mountPath: /opt/airflow/logs
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
      volumes:
      - name: airflow-config
        configMap:
          name: airflow-config
      - name: airflow-logs
        persistentVolumeClaim:
          claimName: airflow-logs-pvc

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: airflow-flower
  namespace: data-platform
  labels:
    app: airflow
    component: flower
    tier: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app: airflow
      component: flower
  template:
    metadata:
      labels:
        app: airflow
        component: flower
        tier: monitoring
    spec:
      containers:
      - name: airflow-flower
        image: apache/airflow:2.8.1-python3.11
        command:
        - /bin/bash
        - -c
        - |
          # Start flower
          airflow celery flower
        env:
        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN
          value: "postgresql+psycopg2://postgres:password123@postgres-primary:5432/airflow"
        - name: AIRFLOW__CORE__FERNET_KEY
          valueFrom:
            secretKeyRef:
              name: airflow-secret
              key: fernet-key
        - name: AIRFLOW__CELERY__BROKER_URL
          value: "redis://:redispassword123@redis:6379/1"
        - name: AIRFLOW__CELERY__RESULT_BACKEND
          value: "redis://:redispassword123@redis:6379/1"
        - name: AIRFLOW__CELERY__FLOWER_HOST
          value: "0.0.0.0"
        - name: AIRFLOW__CELERY__FLOWER_PORT
          value: "5555"
        - name: AIRFLOW__WEBSERVER__SECRET_KEY
          valueFrom:
            secretKeyRef:
              name: airflow-secret
              key: webserver-secret-key
        ports:
        - containerPort: 5555
          name: flower
        volumeMounts:
        - name: airflow-config
          mountPath: /opt/airflow/airflow.cfg
          subPath: airflow.cfg
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
        livenessProbe:
          httpGet:
            path: /
            port: 5555
          initialDelaySeconds: 120
          periodSeconds: 30
        readinessProbe:
          httpGet:
            path: /
            port: 5555
          initialDelaySeconds: 60
          periodSeconds: 10
      volumes:
      - name: airflow-config
        configMap:
          name: airflow-config

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: airflow-logs-pvc
  namespace: data-platform
spec:
  accessModes:
  - ReadWriteOnce
  storageClassName: hostpath
  resources:
    requests:
      storage: 10Gi

---
apiVersion: v1
kind: Service
metadata:
  name: airflow-webserver
  namespace: data-platform
  labels:
    app: airflow
    component: webserver
spec:
  selector:
    app: airflow
    component: webserver
  ports:
  - port: 8080
    targetPort: 8080
    name: webserver
  type: ClusterIP

---
apiVersion: v1
kind: Service
metadata:
  name: airflow-flower
  namespace: data-platform
  labels:
    app: airflow
    component: flower
spec:
  selector:
    app: airflow
    component: flower
  ports:
  - port: 5555
    targetPort: 5555
    name: flower
  type: ClusterIP

---
apiVersion: v1
kind: Service
metadata:
  name: airflow-worker
  namespace: data-platform
  labels:
    app: airflow
    component: worker
spec:
  selector:
    app: airflow
    component: worker
  ports:
  - port: 8793
    targetPort: 8793
    name: worker-log
  clusterIP: None
